/home/sam/github/data-investor-pipeline
├── code_report.py
├── config
├── custom_tree_and_files_corrected.txt
├── .env
├── .env.example
├── .gitignore
├── LICENSE
├── README.md
├── requirements.txt
├── src
│   ├── data_ingestion
│   │   ├── api_financial_modeling_prep
│   │   │   └── fetch_financial_statement.py
│   │   ├── db_connector.py
│   │   └── __init__.py
│   ├── intestion_data_fmp.ipynb
│   ├── run.py
│   └── utils
│       ├── utils_data.py
│       └── utils_logger.py
└── .vscode
    └── tasks.json

7 directories, 16 files


=== Content of /home/sam/github/data-investor-pipeline/.env.example ===

DB_TYPE=LOCAL  # Options: PLANETSCALE, LOCAL, MONGO

FMP_SECRET_KEY=

DB_HOST=
DB_USERNAME=
DB_PASSWORD=
DB_NAME=
  
DB_HOST_LOCAL=localhost
DB_USERNAME_LOCAL=
DB_PASSWORD_LOCAL=
DB_NAME_LOCAL=


MONGO_URI=
MONGO_DB_NAME=
```

=== Content of /home/sam/github/data-investor-pipeline/code_report.py ===

Code present but not reported for space reasons

=== Content of /home/sam/github/data-investor-pipeline/src/run.py ===

import os

from dotenv import load_dotenv
from loguru import logger
from utils.utils_logger import configure_logger

from data_ingestion.db_connector import DBConnector
from data_ingestion.api_financial_modeling_prep.fetch_financial_statement import (
    fetch_and_store_data,
)


def main():
    load_dotenv()

    FMP_SECRET_KEY = os.getenv("FMP_SECRET_KEY")

    config_params = {
        "period": "quarter",
        "apikey": FMP_SECRET_KEY,
        "limit": 20,
        "batch_size": 10,
        "base_url": "https://financialmodelingprep.com/api/v3",
        "store_data": True,
    }
    # Data tables and symbols
    dict_tables = {
        "income-statement": "income_statement",
        "balance-sheet-statement": "balance_sheet_statement",
        "cash-flow-statement": "cash_flow_statement",
    }
    symbols = ["AAPL", "MSFT", "GOOG", "TSLA"]

    # Initialize DBConnector
    db = DBConnector()
    db.initialize_db("MONGO")

    df = fetch_and_store_data(symbols, dict_tables, config_params, db)


if __name__ == "__main__":
    main()


=== Content of /home/sam/github/data-investor-pipeline/src/utils/utils_logger.py ===

import sys


from loguru import logger


def configure_logger():
    """
    Configure the Loguru logger settings.
    """
    logger.remove()
    logger.add(
        lambda msg: sys.stderr.write(msg),
        format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {function}:{line} - {message}",
    )
    logger.add(
        "./logs/data_pipeline.log", rotation="1 day", level="INFO", serialize=True
    )


=== Content of /home/sam/github/data-investor-pipeline/src/utils/utils_data.py ===

from urllib.request import urlopen

import certifi
import json


def get_jsonparsed_data(url):
    response = urlopen(url, cafile=certifi.where())
    data = response.read().decode("utf-8")
    return json.loads(data)


=== Content of /home/sam/github/data-investor-pipeline/src/data_ingestion/db_connector.py ===

import os
import polars as pl
from pymongo import MongoClient
from pymongo.server_api import ServerApi
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from loguru import logger


class DBConnector:
    def __init__(self):
        self.engine = None
        self.SessionLocal = None
        self.mongo_client = None
        self.mongo_db = None

    def connect_sqlalchemy(self, db_url, connect_args=None):
        if connect_args is None:
            connect_args = {}
        try:
            self.engine = create_engine(db_url, connect_args=connect_args)
            self.SessionLocal = sessionmaker(autoflush=False, bind=self.engine)
        except Exception as e:
            logger.error(f"SQLAlchemy connection error: {e}")

    def connect_mongodb(self, uri, db_name):
        try:
            self.mongo_client = MongoClient(uri, server_api=ServerApi("1"))
            self.mongo_db = self.mongo_client[db_name]
            self.mongo_client.admin.command("ping")
            logger.info("Successfully connected to MongoDB")
        except Exception as e:
            logger.error(f"MongoDB connection error: {e}")

    def initialize_db(self, connection_type):
        if connection_type == "PLANETSCALE":
            db_url = self.build_mysql_url()
            ssl_args = {"ssl": {"ca": "/etc/ssl/certs/ca-certificates.crt"}}
            self.connect_sqlalchemy(db_url, ssl_args)

        elif connection_type == "LOCAL":
            db_url = self.build_postgres_url()
            self.connect_sqlalchemy(db_url)

        elif connection_type == "MONGO":
            mongo_uri = os.getenv("MONGO_URI")
            mongo_db_name = os.getenv("MONGO_DB_NAME")
            self.connect_mongodb(mongo_uri, mongo_db_name)

        else:
            raise ValueError("Invalid connection type specified")

    @staticmethod
    def build_mysql_url():
        return f"mysql+pymysql://{os.getenv('DB_USERNAME')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}/{os.getenv('DB_NAME')}"

    @staticmethod
    def build_postgres_url():
        return f"postgresql://{os.getenv('DB_USERNAME_LOCAL')}:{os.getenv('DB_PASSWORD_LOCAL')}@{os.getenv('DB_HOST_LOCAL')}/{os.getenv('DB_NAME_LOCAL')}"

    def write_to_mongo(self, collection_name, df):
        try:
            records = df.to_dicts()
            collection = self.mongo_db[collection_name]
            result = collection.insert_many(records)
            logger.info(
                f"collection_name: {collection_name} - Inserted document IDs: {result.inserted_ids}"
            )
        except Exception as e:
            logger.error(f"Failed to write to MongoDB. Error: {e}")

    def read_from_mongo(self, collection_name, query=None):
        if query is None:
            query = {}
        try:
            collection = self.mongo_db[collection_name]
            data = list(collection.find(query))
            return pl.DataFrame(data)
        except Exception as e:
            logger.error(f"Failed to read from MongoDB. Error: {e}")
            return pl.DataFrame()

    def create_collection(self, db_uri, db_name, collection_name, validator=None):
        if validator is None:
            validator = {}
        try:
            client = MongoClient(db_uri)
            db = client[db_name]
            db.create_collection(collection_name, validator=validator)
            db.command("collMod", collection_name, validator=validator)
        except Exception as e:
            logger.error(f"Error creating collection: {e}")


# # Initialize DBConnector
# db = DBConnector()
# db.initialize_db("MONGO")
# db.create_collection(
#     os.getenv("MONGO_URI"), os.getenv("MONGO_DB_NAME"), "your_collection_name"
# )


=== Content of /home/sam/github/data-investor-pipeline/src/data_ingestion/api_financial_modeling_prep/fetch_financial_statement.py ===

from utils.utils_data import get_jsonparsed_data
from tqdm import tqdm
import polars as pl


def fetch_data_for_symbol(symbol, data_flow, params):
    period = params.get("period")
    api_key = params.get("apikey")
    limit = params.get("limit")

    url = f"{params.get('base_url')}/{data_flow}/{symbol}?period={period}&apikey={api_key}&limit={limit}"

    data = get_jsonparsed_data(url)

    return data


def store_unique_data(df, collection_name, db):
    dedup_fields = [
        "symbol",
        "date",
        "cik",
        "fillingDate",
        "acceptedDate",
        "calendarYear",
        "period",
    ]
    existing_df = db.read_from_mongo(collection_name, {})
    if existing_df.height > 0:
        # Deduplicate against existing data
        unique_data = df.join(existing_df, on=dedup_fields, how="anti")
    else:
        unique_data = df

    if unique_data.height > 0:
        db.write_to_mongo(collection_name, unique_data)


def fetch_and_store_data(symbols, dict_tables, config_params, db):
    # Process symbols in batches of 'batch_size'
    batch_size = config_params.get("batch_size")
    for data_flow in dict_tables.keys():
        for i in range(0, len(symbols), batch_size):
            batch_symbols = symbols[i : i + batch_size]
            batch_data = pl.DataFrame()

            for symbol in tqdm(batch_symbols):
                symbol_data = pl.DataFrame()

                table = dict_tables[data_flow]
                data = fetch_data_for_symbol(symbol, data_flow, config_params)

                if data:
                    df = pl.DataFrame(data)

                    symbol_data = pl.concat([symbol_data, df])

                if symbol_data.height > 0:
                    batch_data = pl.concat([batch_data, symbol_data])

            print(f"{batch_data.height} rows fetched for {data_flow}")
            if config_params.get("store_data"):
                store_unique_data(batch_data, table, db)

    return batch_data
