/home/sam/github/data-driven-investor-platform
├── 1_data_ingestion.ipynb
├── data
├── data_pipeline
│   ├── common
│   │   ├── config.py
│   │   └── constants.py
│   ├── database
│   │   ├── database_session.py
│   │   ├── models.py
│   │   └── utils
│   ├── fetch_data
│   │   └── fetch_equities.py
│   ├── investorkit
│   │   ├── environment.yml
│   │   ├── .gitignore
│   │   ├── investorkit
│   │   │   └── get_data
│   │   │       └── base.py
│   │   ├── LICENSE
│   │   ├── README.md
│   │   └── requirements.txt
│   ├── logs
│   │   └── data_pipeline.log
│   ├── main.py
│   ├── parse_api_fmp.ipynb
│   ├── setup_database.py
│   └── utils
│       ├── context_manager.py
│       └── process_data.py
├── .env
├── .gitignore
├── LICENSE
├── logs
│   └── data_pipeline.log
└── README.md

13 directories, 23 files


=== Content of /home/sam/github/data-driven-investor-platform/data_pipeline/main.py ===

from datetime import datetime
from loguru import logger
from fetch_data.fetch_equities import (
    fetch_and_store_profiles,
    fetch_and_store_financial_statements,
    fetch_and_store_hist_prices,  # Add this line
    fetch_and_store_market_cap_data,  # Add this line
)
from database.database_session import SessionLocal, engine
from common.config import FMP_API_KEY


logger.add("./logs/data_pipeline.log", rotation="1 day", level="INFO", serialize=True)


if __name__ == "__main__":
    run_id = datetime.now().isoformat()
    common_args = {
        "engine": engine,
        "api_key": FMP_API_KEY,
        "SessionLocal": SessionLocal,
        "run_id": run_id,
    }

    try:
        logger.bind(run_id=run_id).info("Starting data pipeline")

        fetch_and_store_profiles(**common_args)
        fetch_and_store_financial_statements(**common_args)
        fetch_and_store_hist_prices(**common_args)  # Add this line
        fetch_and_store_market_cap_data(**common_args)  # Add this line

    except Exception as e:
        logger.bind(run_id=run_id).exception("An error occurred: {}", e)


=== Content of /home/sam/github/data-driven-investor-platform/data_pipeline/setup_database.py ===

from sqlalchemy import create_engine
from database.models import Base
from common.config import (
    DATABASE_URL,
)  # Import from the consolidated config file

engine = create_engine(DATABASE_URL)


def setup_database():
    Base.metadata.create_all(bind=engine)


if __name__ == "__main__":
    setup_database()


=== Content of /home/sam/github/data-driven-investor-platform/data_pipeline/utils/process_data.py ===

import numpy as np
import pandas as pd
from common.constants import MIN_BIGINT, MAX_BIGINT
from loguru import logger


def filter_bigint_range(df: pd.DataFrame, run_id) -> pd.DataFrame:
    try:
        min_bigint, max_bigint = MIN_BIGINT, MAX_BIGINT
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        for col in numeric_cols:
            df = df.query(f"{min_bigint} <= {col} <= {max_bigint}")

        logger.bind(run_id=run_id).info(
            f"Filtered DataFrame based on the bigint range."
        )
        return df
    except Exception as e:
        logger.bind(run_id=run_id).exception(
            f"An error occurred while filtering DataFrame: {e}"
        )
        return pd.DataFrame()


=== Content of /home/sam/github/data-driven-investor-platform/data_pipeline/utils/context_manager.py ===

from contextlib import contextmanager
from sqlalchemy.orm import sessionmaker
from loguru import logger


@contextmanager
def session_scope(SessionLocal, run_id):
    """Provide a transactional scope around a series of operations."""
    session = SessionLocal()
    try:
        yield session
        session.commit()
        logger.bind(run_id=run_id).info("Session committed successfully.")
    except Exception as e:
        session.rollback()
        logger.bind(run_id=run_id).exception(f"An error occurred: {e}")
    finally:
        session.close()


=== Content of /home/sam/github/data-driven-investor-platform/data_pipeline/investorkit/investorkit/get_data/base.py ===

from typing import Union, List, Tuple
import pandas as pd
import numpy as np
from loguru import logger  # Using Loguru for logging

try:
    from tqdm import tqdm

    ENABLE_TQDM = True
except ImportError:
    ENABLE_TQDM = False


### Useful for debug
# tickers=list(df['symbol'])
# statement="cashflow"
# api_key=FMP_API_KEY
# quarter = True
# start_date="2000-01-01"
# end_date=None
# rounding = 4
# progress_bar = True


def get_financial_statements(
    tickers: Union[str, List[str]],
    statement: str = "",
    api_key: str = "",
    quarter: bool = True,
    start_date: Union[str, None] = None,
    end_date: Union[str, None] = None,
    rounding: Union[int, None] = 4,
    progress_bar: bool = True,
) -> Tuple[pd.DataFrame, List[str]]:
    if not isinstance(tickers, (list, str)):
        raise ValueError(f"Invalid type for tickers: {type(tickers)}")

    ticker_list = tickers if isinstance(tickers, list) else [tickers]

    statement_to_location = {
        "balance": "balance-sheet-statement",
        "income": "income-statement",
        "cashflow": "cash-flow-statement",
    }

    location = statement_to_location.get(statement)
    if location is None:
        raise ValueError(
            "Invalid statement type. Choose 'balance', 'income', or 'cashflow'."
        )

    period = "quarter" if quarter else "annual"
    financial_statement_dict = {}
    invalid_tickers = []

    ticker_iterator = (
        tqdm(ticker_list, desc=f"Obtaining {statement} data")
        if ENABLE_TQDM & progress_bar
        else ticker_list
    )

    for ticker in ticker_iterator:
        url = f"https://financialmodelingprep.com/api/v3/{location}/{ticker}?period={period}&apikey={api_key}"

        try:
            financial_statement = pd.read_json(url)
            if financial_statement.empty:
                invalid_tickers.append(ticker)
                continue
        except Exception as error:
            invalid_tickers.append(ticker)
            continue

        date_col = "date" if quarter else "calendarYear"
        freq = "Q" if quarter else "Y"
        financial_statement[date_col] = pd.to_datetime(
            financial_statement[date_col].astype(str)
        ).dt.to_period(freq)

        financial_statement_dict[ticker] = financial_statement

    if not financial_statement_dict:
        return pd.DataFrame(), invalid_tickers

    financial_statement_total = pd.concat(financial_statement_dict)
    financial_statement_total.reset_index(drop=True, inplace=True)
    financial_statement_total = financial_statement_total.drop_duplicates().reset_index(
        drop=True
    )

    if start_date or end_date:
        mask = True
        if start_date:
            mask &= financial_statement_total["date"] >= start_date
        if end_date:
            mask &= financial_statement_total["date"] <= end_date
        financial_statement_total = financial_statement_total[mask]

    financial_statement_total["date"] = financial_statement_total["date"].astype(str)
    return financial_statement_total, invalid_tickers


def get_profile(tickers, api_key):
    if not isinstance(tickers, (list, str)):
        raise ValueError(f"Type for the tickers ({type(tickers)}) variable is invalid.")

    tickers = tickers if isinstance(tickers, list) else [tickers]
    profiles = {}

    for ticker in tqdm(tickers):
        try:
            data = pd.read_json(
                f"https://financialmodelingprep.com/api/v3/profile/{ticker}?apikey={api_key}"
            )
            profiles[ticker] = data
        except Exception as error:
            logger.warning(f"Could not fetch data for {ticker}. Error: {error}")

    profile_dataframe = pd.concat(profiles)
    profile_dataframe = profile_dataframe.reset_index(drop=True)

    return profile_dataframe


def get_historical_market_cap(
    tickers: Union[str, List[str]],
    api_key: str = "",
    start_date: Union[str, None] = None,
) -> pd.DataFrame:
    if not isinstance(tickers, (list, str)):
        raise ValueError(f"Invalid type for tickers: {type(tickers)}")

    ticker_list = tickers if isinstance(tickers, list) else [tickers]
    df_marketcap = pd.DataFrame()

    for ticker in ticker_list:
        url = f"https://financialmodelingprep.com/api/v3/historical-market-capitalization/{ticker}?&apikey={api_key}"
        try:
            data_mod = pd.read_json(url)
            if data_mod.empty:
                continue

            data_mod["date"] = pd.to_datetime(data_mod["date"])

            if start_date:
                start_date_dt = pd.to_datetime(start_date)
                data_mod = data_mod[data_mod["date"] > start_date_dt]

            df_marketcap = pd.concat([df_marketcap, data_mod], ignore_index=True)
        except Exception as error:
            logger.warning(f"Could not fetch data for {ticker}. Error: {error}")

    return df_marketcap


def get_historical_prices(
    tickers: List[str],
    new_tickers: List[str],
    api_key: str,
    start_date: str,
    end_date: str,
) -> pd.DataFrame:
    df_final_price = pd.DataFrame()

    ticker_iterator = tqdm(tickers)
    for ticker in ticker_iterator:
        base_url = "https://financialmodelingprep.com/api/v3/historical-price-full/"
        if ticker not in new_tickers:
            url = f"{base_url}{ticker}?from={start_date}&to={end_date}&apikey={api_key}"
        else:
            url = f"{base_url}{ticker}?from=2000-01-01&to={end_date}&apikey={api_key}"

        try:
            df_price = pd.read_json(url)
            if "historical" in df_price.columns:
                exploded_df = df_price["historical"].apply(pd.Series)
                final_df = pd.concat([df_price["symbol"], exploded_df], axis=1)
                df_final_price = pd.concat(
                    [df_final_price, final_df], ignore_index=True
                )

        except Exception as e:
            logger.warning(f"Could not fetch data for {ticker}. Error: {e}")

    logger.info(f"Shape of historical prices DataFrame: {df_final_price.shape}")

    df_final_price["date"] = pd.to_datetime(df_final_price["date"])

    return df_final_price


=== Content of /home/sam/github/data-driven-investor-platform/data_pipeline/database/models.py ===

from sqlalchemy import (
    Column,
    String,
    Integer,
    Boolean,
    Date,
    Sequence,
    BigInteger,
    DateTime,
)
from sqlalchemy.orm import declarative_base

Base = declarative_base()


# SQLAlchemy Models
class Profile(Base):
    __tablename__ = "profiles"
    symbol = Column(String, primary_key=True, index=True)
    companyName = Column(String)
    cik = Column(Integer)
    exchange = Column(String)
    exchangeShortName = Column(String)
    industry = Column(String)
    sector = Column(String)
    country = Column(String)
    ipoDate = Column(Date)
    defaultImage = Column(Boolean)
    isEtf = Column(Boolean)
    isActivelyTrading = Column(Boolean)


class CashFlow(Base):
    __tablename__ = "cashflows"
    __table_args__ = {"extend_existing": True}

    id = Column(Integer, primary_key=True, autoincrement=True)

    date = Column(String)  # Representing period[Q-DEC] as string
    symbol = Column(String, index=True)
    reportedCurrency = Column(String)
    cik = Column(BigInteger)
    fillingDate = Column(Date)
    acceptedDate = Column(Date)
    calendarYear = Column(BigInteger)
    period = Column(String)

    # Columns changed from Integer to BigInteger
    netIncome = Column(BigInteger)
    depreciationAndAmortization = Column(BigInteger)
    deferredIncomeTax = Column(BigInteger)
    stockBasedCompensation = Column(BigInteger)
    changeInWorkingCapital = Column(BigInteger)
    accountsReceivables = Column(BigInteger)
    inventory = Column(BigInteger)
    accountsPayables = Column(BigInteger)
    otherWorkingCapital = Column(BigInteger)
    otherNonCashItems = Column(BigInteger)
    netCashProvidedByOperatingActivities = Column(BigInteger)
    investmentsInPropertyPlantAndEquipment = Column(BigInteger)
    acquisitionsNet = Column(BigInteger)
    purchasesOfInvestments = Column(BigInteger)
    salesMaturitiesOfInvestments = Column(BigInteger)
    otherInvestingActivites = Column(BigInteger)
    netCashUsedForInvestingActivites = Column(BigInteger)
    debtRepayment = Column(BigInteger)
    commonStockIssued = Column(BigInteger)
    commonStockRepurchased = Column(BigInteger)
    dividendsPaid = Column(BigInteger)
    otherFinancingActivites = Column(BigInteger)
    netCashUsedProvidedByFinancingActivities = Column(BigInteger)
    effectOfForexChangesOnCash = Column(BigInteger)
    netChangeInCash = Column(BigInteger)
    cashAtEndOfPeriod = Column(BigInteger)
    cashAtBeginningOfPeriod = Column(BigInteger)
    operatingCashFlow = Column(BigInteger)
    capitalExpenditure = Column(BigInteger)
    freeCashFlow = Column(BigInteger)

    link = Column(String)
    finalLink = Column(String)


# New model to represent the DataFrame
class MarketCapData(Base):
    __tablename__ = "hist_marketcap"
    id = Column(Integer, primary_key=True, autoincrement=True)

    symbol = Column(String, index=True)
    date = Column(DateTime)
    marketCap = Column(BigInteger)


from sqlalchemy import Float


# New model to represent the DataFrame with trading data
class HistPrice(Base):
    __tablename__ = "hist_prices"
    id = Column(Integer, primary_key=True, autoincrement=True)

    symbol = Column(String, index=True)
    date = Column(DateTime)
    open = Column(Float)
    high = Column(Float)
    low = Column(Float)
    close = Column(Float)
    adjClose = Column(Float)
    volume = Column(BigInteger)
    unadjustedVolume = Column(BigInteger)
    change = Column(Float)
    changePercent = Column(Float)
    vwap = Column(Float)
    label = Column(String)
    changeOverTime = Column(Float)


=== Content of /home/sam/github/data-driven-investor-platform/data_pipeline/database/database_session.py ===

from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from loguru import logger
from common.config import DATABASE_URL

try:
    engine = create_engine(DATABASE_URL)
    SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
    logger.info("Successfully initialized the database session.")
except Exception as e:
    logger.exception(f"An error occurred while initializing the database: {e}")


=== Content of /home/sam/github/data-driven-investor-platform/data_pipeline/common/config.py ===

from dotenv import load_dotenv
import os
from loguru import logger  # Using Loguru for logging

# Load environment variables
load_dotenv()

try:
    DATABASE_URL = os.getenv("DATABASE_URL")
    FMP_API_KEY = os.getenv("FMP_SECRET_KEY")

    if DATABASE_URL is None:
        logger.warning("DATABASE_URL environment variable is missing.")
    if FMP_API_KEY is None:
        logger.warning("FMP_SECRET_KEY environment variable is missing.")

    logger.info("Successfully loaded environment variables.")
except Exception as e:
    logger.exception("Failed to load environment variables: {}", e)


=== Content of /home/sam/github/data-driven-investor-platform/data_pipeline/common/constants.py ===

MIN_BIGINT = -9223372036854775808
MAX_BIGINT = 9223372036854775807


=== Content of /home/sam/github/data-driven-investor-platform/data_pipeline/fetch_data/fetch_equities.py ===

import financedatabase as fd
from investorkit.investorkit.get_data.base import (
    get_profile,
    get_financial_statements,
    get_historical_market_cap,
    get_historical_prices,
)

from utils.context_manager import session_scope
from utils.process_data import filter_bigint_range
from datetime import datetime
import pandas as pd
from loguru import logger

# Add the run_id to logger context
run_id = datetime.now().isoformat()
logger.bind(run_id=run_id)


def store_to_db(df, table_name, engine, SessionLocal, run_id):
    try:
        with session_scope(SessionLocal, run_id) as session:
            df.to_sql(table_name, con=engine, if_exists="append", index=False)
            session.flush()
        logger.bind(run_id=run_id).info(f"Stored data into table {table_name}")
    except Exception as e:
        logger.bind(run_id=run_id).error(
            f"Failed to store data into table {table_name}. Error: {e}"
        )


def fetch_equity_symbols(country="United States", market="NASDAQ Global Select"):
    try:
        equities = fd.Equities()
        selected_columns = [
            "name",
            "currency",
            "sector",
            "industry_group",
            "industry",
            "exchange",
            "market",
            "market_cap",
        ]
        us_equities = equities.select(country=country)
        df_equities = us_equities[us_equities["market"] == market][selected_columns]
        list_symbols = list(df_equities.index)
        logger.info(
            f"Fetched {len(list_symbols)} equity symbols for {country} - {market}"
        )
        return list_symbols
    except Exception as e:
        logger.exception(
            f"Failed to fetch equity symbols for {country} - {market}. Error: {e}"
        )


def get_new_symbols(list_symbols, engine):
    try:
        existing_symbols_query = "SELECT symbol FROM profiles;"
        existing_symbols = pd.read_sql(existing_symbols_query, con=engine)
        new_symbols = list(set(list_symbols) - set(existing_symbols["symbol"].tolist()))

        logger.info(f"Identified {len(new_symbols)} new symbols")
        return new_symbols
    except Exception as e:
        logger.exception(f"Failed to identify new symbols. Error: {e}")


def fetch_and_store_profiles(engine, api_key, SessionLocal, run_id):
    list_symbols = fetch_equity_symbols()
    new_symbols = get_new_symbols(list_symbols, engine)

    if new_symbols:
        df_profiles = get_profile(new_symbols, api_key)
        if not df_profiles.empty:
            list_cols = [
                "symbol",
                "companyName",
                "cik",
                "exchange",
                "exchangeShortName",
                "industry",
                "sector",
                "country",
                "ipoDate",
                "defaultImage",
                "isEtf",
                "isActivelyTrading",
            ]
            df_profiles_filtered = df_profiles[list_cols]
            df_profiles_filtered["ipoDate"].replace("", None, inplace=True)
            df_profiles_filtered["cik"].replace("", None, inplace=True)
            store_to_db(df_profiles_filtered, "profiles", engine, SessionLocal, run_id)
        else:
            logger.warning("No profiles found for the new symbols.")


def fetch_and_store_financial_statements(engine, api_key, SessionLocal, run_id):
    try:
        query = "SELECT DISTINCT symbol FROM cashflows;"
        existing_symbols_df = pd.read_sql(query, engine)
        existing_symbols = set(existing_symbols_df["symbol"])

        query = "SELECT * FROM profiles;"
        df_profiles = pd.read_sql(query, engine)

        list_symbols = list(df_profiles["symbol"])
        list_symbols = [
            symbol for symbol in list_symbols if symbol not in existing_symbols
        ]

        chunks = [list_symbols[i : i + 100] for i in range(0, len(list_symbols), 100)]

        for chunk in chunks:
            df, invalid_tickers = get_financial_statements(
                tickers=chunk,
                statement="cashflow",
                api_key=api_key,
                start_date="2000-01-01",
            )
            filtered_df = filter_bigint_range(df, run_id)

            store_to_db(filtered_df, "cashflows", engine, SessionLocal, run_id)
            logger.info(f"Stored financial statements for {len(chunk)} symbols.")
    except Exception as e:
        logger.exception(
            f"An error occurred while fetching and storing financial statements: {e}"
        )


def fetch_and_store_hist_prices(engine, api_key, SessionLocal, run_id):
    try:
        # Fetch existing symbols from the database
        query = "SELECT DISTINCT symbol FROM hist_prices;"
        existing_symbols_df = pd.read_sql(query, engine)
        existing_symbols = set(existing_symbols_df["symbol"])

        query = "SELECT * FROM profiles;"
        df_profiles = pd.read_sql(query, engine)

        list_symbols = list(df_profiles["symbol"])
        list_symbols = [
            symbol for symbol in list_symbols if symbol not in existing_symbols
        ]

        # Fetch and store historical prices
        df = get_historical_prices(
            list_symbols, [], api_key, "2000-01-01", "2023-10-17"
        )
        filtered_df = filter_bigint_range(df, run_id)

        store_to_db(filtered_df, "hist_prices", engine, SessionLocal, run_id)
        logger.info(f"Stored historical prices for {len(list_symbols)} symbols.")
    except Exception as e:
        logger.exception(
            f"An error occurred while fetching and storing historical prices: {e}"
        )


def fetch_and_store_market_cap_data(engine, api_key, SessionLocal, run_id):
    try:
        # Fetch existing symbols from the database
        query = "SELECT DISTINCT symbol FROM hist_marketcap;"
        existing_symbols_df = pd.read_sql(query, engine)
        existing_symbols = set(existing_symbols_df["symbol"])

        query = "SELECT * FROM profiles;"
        df_profiles = pd.read_sql(query, engine)

        list_symbols = list(df_profiles["symbol"])
        list_symbols = [
            symbol for symbol in list_symbols if symbol not in existing_symbols
        ]

        # Divide the list_symbols into chunks
        chunks = [list_symbols[i : i + 100] for i in range(0, len(list_symbols), 100)]

        for chunk in chunks:
            # Fetch and store historical prices for each chunk
            df = get_historical_market_cap(chunk, api_key, "2000-01-01")
            filtered_df = filter_bigint_range(df, run_id)

            store_to_db(filtered_df, "hist_marketcap", engine, SessionLocal, run_id)
            logger.info(f"Stored historical prices for {len(chunk)} symbols.")

    except Exception as e:
        logger.exception(
            f"An error occurred while fetching and storing market cap data: {e}"
        )
